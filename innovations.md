# Recent developments in scholarly publishing: a view from the life sciences

Stephen J Eglen [(orcid)](https://orcid.org/0000-0001-8607-8025),
Ross Mounce [(orcid)](https://orcid.org/0000-0002-3520-2046),
Laurent Gatto [(orcid)](https://orcid.org/0000-0002-1520-2268),
Adrian M Currie [(orcid)](https://orcid.org/0000-0003-2638-202X),
Yvonne Nobis [(orcid)](https://orcid.org/0000-0001-9147-7418).

# Introduction

In some ways, scholarly publishing has not changed much in the last
ten years.  Publishing in prestigious top-tier journals is still
perceived as critical for career progression (especially gaining
promotion and grants).  Likewise, journal metrics continue to dominate
in the evaluation of a paper’s research, rather than the paper’s
contents [@Brenner1995-kk; @Simons2008-fr]. Against the backdrop of
highly competitive job and grant markets, factors such as these
encourage narrow research agendas and tie researchers (particularly in
early career) to placing work in exploitative publishers who draw
significant funds from academic work. Further, standard publishing
criteria, especially for instance on publishing statistically
significant, positive results, creates biases across published
studies. However, there are several reasons for optimism that the
nature of scientific publishing will improve. Here we outline some
recent developments in the life sciences.



## Preprints

Since 1991 ArXiv [@Ginsparg2017] has become a standard tool for
physicists to rapidly disseminate their research findings.  On the
surface, it does not provide much beyond a collection of PDFs grouped
by topic.  In certain fields, being the first to publish on the ArXiv
is considered to be the key step, although subsequent journal
publication is still the norm.  Although ArXiv hosts papers in
quantitative biology, it was initially assumed that biologists would
not adopt a preprint culture: publishing a preprint might prevent
subsequent publication in a top-tier journal, or leading to scooping
by another group.  There is some historic
justification of these concerns: an NIH experiment in preprints was
effectively halted in the 1960s by journals' refusal to accept
preprints for submission [@Cobb2017-tv].

BioRxiv, launched in 2013, has overcome these concerns. Researchers in
diverse areas as ecology, neuroscience and genomics are uploading
preprints and choosing to share their work ahead of publication.
There are many reasons for this usage:

* Sharing work before submitting to a journal allows for community
  feedback.
* Sharing work at the time of submission means that the community can
  read the work months (or years) before the work eventually
  appears in print.
* Journal editors browse bioRxiv and suggest relevant papers be
  submitted to their journal.
  
* BioRxiv preprints can be transferred rapidly to journal submission
  systems rather than going through (an often lengthy) direct submission
  to the journal.
* Several funding agencies, including NIH and UKRI, allows
  preprints to be listed on CVs and cited in grant applications.

Several other preprint servers are available, in particular [PeerJ
Preprints](https://peerj.com/preprints/), [OSF Preprint
servers](https://osf.io/preprints/), and
[preprints.org](https://preprints.org), although to date BioRxiv is
the dominant repository.  Unlike a few years ago, most journals in the
life sciences no longer see prior appearance in a preprint repository as a
block to formal publication.

## Overcoming the reproducibility crisis

According to recent surveys, life scientists across many domains
believe there is a "reproducibility crisis" in science: i.e. many key
findings in publications are either not independently verified, or
fail verification when it is done [@Baker2016-wr].  The traditional publishing
system must take some responsibility for these low levels of
reproducibility.  However, here we list four encouraging developments
that should promote reproducibility.

**Preregistration papers** typically describe the introduction and
methods sections of a study, and are peer-reviewed *before* the study
is actually performed [@Nosek2018-my].  This allows reviewers to
improve the study design and commits researchers to hypotheses that
they wish to study along with their statistical analysis.  Once the
pre-registration study is approved, it is then published.  After the
research is completed, another paper describes the results of the
study using the pre-registered methods.  (Additional findings can be
reported, but are clearly marked as such.)  Preregistration is most
prevalent today in psychology; The Center for Open Science
Preregistration Challenge <https://cos.io/prereg/> is helping to
popularise this notion more broadly.  Initial analysis of results from
preregistered papers indicates, perhaps reassuringly, a marked increase
in null results reported [@Warren2018-cj].

**Stronger data sharing policies and community expectations.** Both
funders and journals are now making stronger statements about what
research materials (data, computer programs, reagents) should be
shared upon publication of the corresponding articles.  Although these
policies should increase data availability and reuse, the current
compliance rates are quite low [@Federer2018-qg].  Given that it
can take considerable time and effort (for both researchers and
journals) to ensure data is appropriately shared, these low-uptake
rates are perhaps expected.  To reward authors for this work, "data
papers" (a paper  that simply describes the data) are becoming more
prominent, e.g. in journals like *Scientific Data* and *Gigascience*.

**Reproducible manuscripts** are documents that contain the main text
as well as the code to generate tables, figures and results have been
around for decades, and have been widely used in many research fields
[@Buckheit1995-yu]. However, even though researchers have been
committed to reproducible research, the reproducibility of the final
outputs were generally broken upon submission to journals. Researchers
have released reproducible versions of their work in parallel to the
journal articles [@Gatto2014-wc; @Breckels2018-ys].  Recently, some
journals have moved closer to publishing reproducible manuscripts, by
working towards a reproducible document stack [@web:elifestack] 
or supporting reproducible figures [@web:biochannel].

**Replicability studies** Given the (often intense) competition to be
first to publish in some areas of biology, being "scooped" on
publishing a particular result can be doubly damaging.  Not only does
someone else publish the result first, your manuscript is often no
longer regarded as novel, and thus not worthy of publication by top-tier
journals.  However, given that science relies on the gradual
accumulation of evidence over a large body of papers, such replication
studies are valuable.  In January 2018, *PLOS Biology* announced that
they would consider for publication those papers that "confirm or
extend a recently published study"
[@PLOS_Biology_Staff_Editors2018-jf].  In a similar vein,
replicability studies can provide clear evidence to evaluate
controversial findings [@Aizawa2016-lu].

## Other recent innovations of note


**ORCiD** <https://orcid.org> provides a persistent, unique digital identifier for
researchers.  Many journals now require that at least one author
verifies their identity as author using ORCiD [@web:orcidletter].

**DORA** [@web:dora] is a declaration for individuals and
institutions to commit to evaluating research based on its content
rather than metrics.  Most UK funders have signed, although only a few
universities have signed. See also the Leiden Manifesto for Research
Metrics (<http://www.leidenmanifesto.org/>).

**Published peer review reports**.  Many journals now already or have
pledged to provide greater transparency about the quality of peer
review they provide by publishing the content of the reviewer reports
alongside published articles. Notably two large open access publishers
PLOS and MDPI are amongst those that are pledging to provide greater
transparency from 2019 [@asap_letter].  PubPeer
<https://pubpeer.com/> allows reviewers to 'claim' metadata records on
their profile for peer reviewing and editorial work they have done.

**Post publication peer review**.  A journal may immediately publish a
paper upon submission; reviews are then sought for the preprint and
made public.  If sufficient reviewers support publication, the
article is formally accepted and e.g. listed on Pubmed.  Leading examples of
this approach are F1000 Research, who provide the infrastructure for
several institution- and funder-specific journals, such as *Wellcome
Open Research* and *Gates Open Research*.

**Format free submissions**.  Journals have traditionally imposed
strict formatting requirements for manuscripts before peer review.
As editors at top-tier journals 'desk reject' most submissions before
peer review, this leads to many wasted hours [@Budd2017-gd].  Gradually
life science journals are now dropping these formatting requirements
for initial submissions, instead allowing "format free" submissions
[@Khan2018-zm].

## Funder mandates and compliance

Key funders in the UK have had policies in place supporting open
access for many years.  In particular, the Wellcome Trust has mandated
Open Access for publications funded by them since 2006, with sanctions
for non-compliance.  Compliance rates (around 90%) are highest for
Wellcome, as of October 2017 [@Lariviere2018-nc], with compliance for
other main funders varying at 70-90%.  Where work has been supported
by relevant funding agencies, our experience to date is that funds
have always been available to support Article Processing Charges
(APCs).  However, one of us [SJE] has experienced difficulties in
finding APCs for papers summarising work supported by internal, rather
than external, funds.


One perhaps unintended consequence of these policies has been that
most traditional journals have established a "hybrid" model of
publishing, with APCs that on average exceed those in pure Open Access
(OA) journals [@Pinfield2017-qs].  This hybrid model of publishing has
shown little signs to date of disappearing, as e.g. funds from
Wellcome Trust have supported high APCs. The success of OA publishing
however has meant that government-provided funds can often no longer
cover all APCs and UK institutions are beginning to restrict the
choice of journals for which APCs will be paid.  However, the OA
publishing world is due to change dramatically in 2020 with the recent
announcement of "plan S" [@Schiltz2018-jn], a European initiative to
enforce OA, cap APCs and prohibit publishing in hybrid journals.
Whilst we support the notions underlying plan S, its success will
depend on further implementation details (e.g. the nature of the APC
cap, recognition of green and diamond OA).



# Concluding remarks

Current publication practices can often lead early career researchers
to be ‘Bullied into Bad Science’ (<http://bulliedintobadscience.org/>).
We have outlined several recent developments that we hope present
alternatives to the traditional hierarchy of scholarly publishing.
These developments should help reduce the pressure on early career
researchers that they currently face in the "publish or perish"
culture.  We encourage the adoption of the above open practices to
help create a more ethical research environment.


# Conflicts of interest

Stephen J Eglen [(orcid)](https://orcid.org/0000-0001-8607-8025),
Ross Mounce [(orcid)](https://orcid.org/0000-0002-3520-2046),
Laurent Gatto [(orcid)](https://orcid.org/0000-0002-1520-2268),
Adrian M Currie [(orcid)](https://orcid.org/0000-0003-2638-202X),
Yvonne Nobis [(orcid)](https://orcid.org/0000-0001-9147-7418).

SJE, RM, LG and AMC are members of the Bullied into Bad Science
campaign.  SJE is on the editorial board of *Scientific Data*.

# Acknowledgements

SJE thanks Magdalene College Cambridge for financial support.

<!-- # Glossary/abbreviations -->


<!-- Perhaps need a list of key abbreviations/terms that are jargon -->
<!-- (APCs/hybrid/diamond OA). -->

# References

<!--
notes (not for paper)

removed text about overlay journal...
BioRxiv can even be used as the substrate for an overlay journal
(example?), and we look forward to the creation of prominent diamond
OA overlay journals in the life sciences.

**Badges** to promote sharing of resources, rather than just the
papers.  e.g. mention data papers?  (Not sure whether to include
this.) (Ross: I'm not that enthused about badges https://blogs.plos.org/absolutely-maybe/2017/08/29/bias-in-open-science-advocacy-the-case-of-article-badges-for-data-sharing/ )

ORCID This will undoubtedly help reduce many types of authorship fraud (cite http://nautil.us/issue/42/fakes/why-fake-data-when-you-can-fake-a-scientist ?)

Crossref can now register DOIs for peer
review reports as a distinct content type, and formally link these to
the DOIs of the articles they review
(https://www.crossref.org/news/2018-06-05-introducing-metadata-for-peer-review/).

Nice figure for preprint usage at: http://www.prepubmed.org/monthly_stats/
Anon (2017) Are preprints the future of biology? A survival guide for
scientists. Science | AAAS Available at:
https://www.sciencemag.org/news/2017/09/are-preprints-future-biology-survival-guide-scientists

 -->

<!--  LocalWords:  PDFs ArXiv preprint preprints BioRxiv Mounce Gatto
 -->
<!--  LocalWords:  Currie reproducibility funders funder APCs OA SJE
 -->
