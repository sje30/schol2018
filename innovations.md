# Recent developments in scholarly publishing to improve research practices in the life sciences

\points{R1b}

Stephen J Eglen [(orcid)](https://orcid.org/0000-0001-8607-8025),
Ross Mounce [(orcid)](https://orcid.org/0000-0002-3520-2046),
Laurent Gatto [(orcid)](https://orcid.org/0000-0002-1520-2268),
Adrian M Currie [(orcid)](https://orcid.org/0000-0003-2638-202X),
Yvonne Nobis [(orcid)](https://orcid.org/0000-0001-9147-7418).


<!-- now in review; check out github issues if revisions allowed -->

# Introduction

In some ways, scholarly publishing has not changed much in the last
ten years.  Publishing in prestigious top-tier journals is still
perceived as critical for career progression (especially gaining
promotion and grants).  Likewise, journal metrics continue to dominate
in the evaluation of a paper’s research, rather than the paper’s
contents [@Brenner1995-kk; @Simons2008-fr]. Against the backdrop of
highly competitive job and grant markets, factors such as these
encourage narrow research agendas and tie researchers (particularly in
early career) to placing work in exploitative publishers who draw
significant funds from academic work. Further, standard publishing
criteria, especially for instance on publishing statistically
significant, positive results, creates biases across published
studies. However, there are several reasons for optimism that the
nature of scientific publishing will improve. Here we outline some
recent developments \points{R1a}*that we believe will improve
the working environment and career prospects for life scientists*.



## Preprints

Since 1991 ArXiv [@Ginsparg2017] has become a standard tool for
physicists to rapidly disseminate their research findings.  Although
on the surface ArXiv does not provide much more than a collection of
PDFs grouped via topic, publishing there is now considered key for
establishing priority in certain fields (subsequent journal
publication is still the norm).  It was initially assumed that
biologists would not adopt a preprint culture: publishing a preprint
might prevent subsequent publication in a top-tier journal, or leading
to scooping by another group.  There is some historic justification of
these concerns: an NIH experiment in preprints was effectively halted
in the 1960s by journals' refusal to accept preprints for submission
[@Cobb2017-tv].

BioRxiv, launched in 2013, has overcome these concerns. Researchers in
such diverse areas as ecology, neuroscience and genomics are uploading
preprints and choosing to share their work ahead of publication.
There are many reasons for this usage:

* Sharing work before submitting to a journal allows for community
  feedback.
* Sharing work at the time of submission means that the community can
  read the work months (or years) before the work eventually
  appears in print.
* Journal editors browse bioRxiv and suggest relevant papers be
  submitted to their journal.
  
* BioRxiv preprints can be transferred rapidly to journal submission
  systems rather than going through (an often lengthy) direct submission
  to the journal.
* Several funding agencies, including NIH and UKRI, allows
  preprints to be listed on CVs and cited in grant applications.

Several other preprint servers are available, in particular [PeerJ
Preprints](https://peerj.com/preprints/), [OSF Preprint
servers](https://osf.io/preprints/), and
[preprints.org](https://preprints.org), although to date BioRxiv is
the dominant repository.  Unlike a few years ago, most journals in the
life sciences no longer see prior appearance in a preprint repository as a
block to formal publication.

## Overcoming the reproducibility crisis

According to recent surveys, life scientists across many domains
believe there is a "reproducibility crisis" in science: i.e. many key
findings in publications are either not independently verified, or
fail verification when it is attempted [@Baker2016-wr].  The
traditional publishing system must take some responsibility for these
low levels of reproducibility, \points{R1.2}*as authors feel under
intense pressure to publish to avoid being scooped (see below).
Further, space limitations imposed by print journals inhibit adequate
method descriptions, with key details relegated to supplementary
information which is rarely scrutinised to the same degree as the main
paper.*  However, here we list four encouraging developments that should
promote reproducibility.

**Preregistration papers** typically describe the introduction and
methods sections of a study, and are peer-reviewed *before* the study
is actually performed [@Nosek2018-my].  This allows reviewers to
improve the study design and commits researchers to hypotheses that
they wish to study along with their statistical analysis.  Once the
pre-registration study is approved, it is then published.  After the
research is completed, another paper \points{R1.3a}*can be submitted to the same
journal* which describes the results of the study using the
pre-registered methods.  (Additional findings can be reported, but are
clearly marked as such.)  \points{R1.3b}*Reviewer and editorial
decisions on whether to accept the second paper is made on the
technical correctness of the paper rather than the importance or
novelty of the results.* Preregistration is most prevalent today in
psychology; The Center for Open Science Preregistration Challenge
<https://cos.io/prereg/> is helping to popularise this notion more
broadly.  Initial analysis of results from preregistered papers
indicates, perhaps reassuringly, a marked increase in null results
reported [@Warren2018-cj].

**Stronger data sharing policies and community expectations.** Both
funders and journals are now making stronger statements about what
research materials (data, computer programs, reagents) should be
shared upon publication of the corresponding articles.  Although these
policies should increase data availability and reuse, compliance rates
are fairly low [@Federer2018-qg].  Given that it can take considerable
time and effort (for both researchers and journals) to ensure data is
appropriately shared, these low-uptake rates are perhaps expected.  To
reward authors for this work, "data papers" (a paper that simply
describes the data) are becoming more prominent, e.g. in journals like
*Scientific Data* and *Gigascience*.  *Journals are also providing
guidelines for authors to follow which should improve reproducibility
and transparency [@Nosek2015-og].* \points{R1.6}

**Reproducible manuscripts** are documents that contain the main text
as well as the code to generate tables, figures and results
[@Buckheit1995-yu]. However, even though researchers have been
committed to reproducible research, the reproducibility of the final
outputs were generally broken upon submission to journals. Researchers
have released reproducible versions of their work in parallel to the
journal articles [@Gatto2014-wc; @Breckels2018-ys].  Recently, some
journals have moved closer to publishing reproducible manuscripts, by
working towards a reproducible document stack [@web:elifestack] or
supporting reproducible figures [@web:biochannel].  *Further, some
journals are experimenting with re-running of code "in the cloud"
using services such as Cloud Ocean [@web:codeocean].* \points{R1.4}

**Replicability studies** Given the (often intense) competition to be
first to publish in some areas of biology, being "scooped" on
publishing a particular result can be doubly damaging.  Not only does
someone else publish the result first, your manuscript is often no
longer regarded as novel, and thus not worthy of publication by top-tier
journals.  However, given that science relies on the gradual
accumulation of evidence over a large body of papers, such replication
studies are valuable.  In January 2018, *PLOS Biology* announced that
they would consider for publication those papers that "confirm or
extend a recently published study"
[@PLOS_Biology_Staff_Editors2018-jf].  In a similar vein,
replicability studies can provide clear evidence to evaluate
controversial findings [@Aizawa2016-lu].

## Other recent innovations of note


**ORCiD** provides a persistent, unique digital identifier for
researchers which can help in linking scholarly outputs to an
individual *and thus automatically curate a list of works, not just
papers, in one place.  Such indentifiers should also reduce mistaken
identity.* \points{R1.5a}.  Many journals now require that
at least one author verifies their identity as author using ORCiD
[@web:orcidletter].  


**DORA** [@web:dora] is a declaration for individuals and
institutions to commit to evaluating research based on its content
rather than metrics.  Most UK funders have signed, although only a few
universities have currently signed. See also the Leiden Manifesto for Research
Metrics (<http://www.leidenmanifesto.org/>).  *We expect most UK
universities to now sign DORA, or equivalent, due to the Wellcome
Trust's revised policy (November 2018) requiring institutions to
sign.  Assuming institutions take this seriously, evaluating papers by
their content rather than where they are published should reduce the
pressure to publish in top-tier journals.*  \points{R1.5b}

**Published peer review reports**.  Many journals now already or have
pledged to provide greater transparency about the quality of peer
review they provide by publishing reviewer reports alongside published
articles. Notably two large open access publishers PLOS and MDPI are
amongst those that are pledging to provide greater transparency from
2019 [@asap_letter].  *Publons* \points{O1} (<https://publons.com/>) allows
reviewers to 'claim' metadata records on their profile for peer
reviewing and editorial work they have done.  *Publishing reviewer
reports, whether signed or not, should increase transparency in the
reviewing process.* \points{R1.5c}

**Post publication peer review**.  A journal may immediately publish a
paper upon submission; reviews are then sought for the preprint and
made public.  If sufficient reviewers support publication, the
article is formally accepted and e.g. listed on Pubmed.  Leading examples of
this approach are F1000 Research, who provide the infrastructure for
several institution- and funder-specific journals, such as *Wellcome
Open Research* and *Gates Open Research*.  \points{R1.5d}*This
approach to publishing complements preprinting by ensuring the paper
is publicly available whilst undergoing peer review.*

**Format free submissions**.  Journals have traditionally imposed
strict formatting requirements for manuscripts.  As editors at
top-tier journals 'desk reject' most submissions before peer review,
this leads to many wasted hours [@Budd2017-gd].  Gradually life
science journals are now dropping these formatting requirements for
initial submissions, instead allowing "format free" submissions
[@Khan2018-zm], *and hence saving researchers from tedious
reformatting tasks* \points{R1.5e}.

## Funder mandates and compliance

Key funders in the UK have had policies in place supporting open
access for many years.  In particular, the Wellcome Trust has mandated
Open Access for publications funded by them since 2006, with sanctions
for non-compliance.  Compliance rates (around 90%) are highest for
Wellcome, as of October 2017 [@Lariviere2018-nc], with compliance for
other main funders varying at 70-90%.  Where work has been supported
by relevant funding agencies, our experience to date is that funds
have always been available to support Article Processing Charges
(APCs).  

One unintended consequence of these policies has been that most
traditional journals have established a "hybrid" model of publishing,
with APCs that on average exceed those in pure Open Access (OA)
journals [@Pinfield2017-qs].  This hybrid model of publishing has so
far shown little signs of disappearing.  The success of OA publishing
however has meant that government-provided funds can often no longer
cover all APCs and UK institutions are beginning to restrict the
choice of journals for which APCs will be paid-to in order to best
optimise the allocation of limited financial resources. We understand
and are supportive of institutions that do not allow APCs to be paid
for hybrid-OA, or for particularly expensively priced OA journals -
some publishing options are simply exploitative of the system and
authors may need protection from them.

However, the OA publishing world is due to change dramatically in 2020
with the recent announcement of "plan S" [@Schiltz2018-jn], a European
initiative to enforce OA, cap APCs and prohibit publishing in hybrid
journals.  Whilst we support the notions underlying plan S, its
success will depend on further implementation details \points{R2.1}
*that are currently under discussion.  To date, gold OA, where authors
typically pay often quite large APCs, is seen as the predominant way
of meeting funders' OA mandates.  However green OA (publishing your
author-accepted manuscript on a suitable server) is a viable
alternative to making work freely available.  Finally, a relatively
new model of diamond OA, where there are no fees either to read or
publish papers, is being explored.  This approach has been
successfully used in mathematics, where costs are kept low by hosting
the published papers on ArXiv.  With the rise of preprint servers in
the life sciences, we look forward to the emergence of similar
low-cost overlay journals in the life sciences.*




# Concluding remarks

Current publication practices can often lead early career researchers
to be ‘Bullied into Bad Science’ (<http://bulliedintobadscience.org/>).
We have outlined several recent developments that we hope present
alternatives to the traditional hierarchy of scholarly publishing.
These developments should help reduce the pressure on early career
researchers that they currently face in the "publish or perish"
culture.  We encourage the adoption of the above open practices to
help create a more ethical research environment.


# Conflicts of interest

SJE, RM, LG and AMC are members of the Bullied into Bad Science
campaign.  SJE is on the editorial board of *Scientific Data* and an
affiliate of *BioRxiv*. \points{O2}

# Acknowledgements

SJE thanks Magdalene College Cambridge for financial support.

<!-- # Glossary/abbreviations -->


<!-- Perhaps need a list of key abbreviations/terms that are jargon -->
<!-- (APCs/hybrid/diamond OA). -->

# References

<!--
notes (not for paper)

removed text about overlay journal...
BioRxiv can even be used as the substrate for an overlay journal
(example?), and we look forward to the creation of prominent diamond
OA overlay journals in the life sciences.

**Badges** to promote sharing of resources, rather than just the
papers.  e.g. mention data papers?  (Not sure whether to include
this.) (Ross: I'm not that enthused about badges https://blogs.plos.org/absolutely-maybe/2017/08/29/bias-in-open-science-advocacy-the-case-of-article-badges-for-data-sharing/ )

ORCID This will undoubtedly help reduce many types of authorship fraud (cite http://nautil.us/issue/42/fakes/why-fake-data-when-you-can-fake-a-scientist ?)

Crossref can now register DOIs for peer
review reports as a distinct content type, and formally link these to
the DOIs of the articles they review
(https://www.crossref.org/news/2018-06-05-introducing-metadata-for-peer-review/).

Nice figure for preprint usage at: http://www.prepubmed.org/monthly_stats/
Anon (2017) Are preprints the future of biology? A survival guide for
scientists. Science | AAAS Available at:
https://www.sciencemag.org/news/2017/09/are-preprints-future-biology-survival-guide-scientists

 -->

<!--  LocalWords:  PDFs ArXiv preprint preprints BioRxiv Mounce Gatto
 -->
<!--  LocalWords:  Currie reproducibility funders funder APCs OA SJE
 -->
